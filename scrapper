#imports
import json, time, requests
from bs4 import BeautifulSoup

# definition start URL

start_url = "https://www.marmiton.org/recettes/index/ingredient"
food = []
page = requests.get(start_url)
soup = BeautifulSoup(page.content, "html.parser")
results = soup.find(id="content")

# Catch alphabet letters in all pages 
alphabet = results.find_all("div", class_="center-text")
alphabet_list = []

#Brows et and catch letters in index's ingredients
for letter_alphabet in alphabet:
     time.sleep(2)
     letterlist = letter_alphabet.find("ul",)# récupère tous les li dans le ul situé dans center-text
     convert = letterlist.text.strip() #conserve uniquement la valeur du li
     alphabet_list.append(convert) #Add letter to continue in letter_alphabet

# Creation of URL whith concatenation (www.blabla + letter)
for i in convert:
     time.sleep(2)
     if i != "\n": #Catch only strings 
          l = i # letter variable in URL 

          # BRowse pagination for catch numbers of pages (1, 2, 3, 4 ...)
          subPageNumber = results.find_all("nav", class_="af-pagination")
          numbersList = []

          for number in subPageNumber:
               time.sleep(2)
               num = number.find("ul",)
               numContent = num.text.strip()

          #insert résults of pagination in list
          for n in numContent:
               numbersList.append(n)
               urlUpdate = f"{start_url}/{l}/{n}" # l is equal to letter and n is equal to pagination number

               page = requests.get(urlUpdate)
               soup = BeautifulSoup(page.content, "html.parser")
               results = soup.find(id="content")
               ingredients = results.find_all("div", class_="index-item-card")  # recipe-results fix-inline-block


               # Look for class showMore. showmore open list with more paginations
               url = "https://www.marmiton.org/recettes/index/ingredient/p"
               target = requests.get(url)
               soup = BeautifulSoup(target.content, "html.parser")
               moreNumbers = soup.find_all("div", "showMorePages")
               lst = []

               if moreNumbers != None:
                    for moreNum in moreNumbers:
                         num = moreNum.find_all("li", )
                         lst.extend(num)
                    print(lst)


               for ingredient in ingredients:
                    ing = ingredient.find("div", class_="index-item-card-name")  # index-item-card-name
                    food.append(ing.text.strip())
                    print(food)
                    #time.sleep(2)


               # écrire les ingrédients dans un fichier texte Json avec écriture normalisée
                    with open("ingrédients.json", "w", encoding="utf-8") as file:
                         for i in food:
                              json.dump(i.capitalize(), file, ensure_ascii=False)
